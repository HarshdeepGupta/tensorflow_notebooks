{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# declare some varibles\n",
    "num_datapoints = 200\n",
    "batch_size = 50\n",
    "steps = 10000\n",
    "# learn a cubic function \n",
    "a = 1 ; b  = -4 ; c = 6; d= -9\n",
    "\n",
    "learn_rate = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Polynomial Regression Model\n",
    "X = tf.placeholder(tf.float32,[None, 1], name=\"Data_matrix_row\")\n",
    "W = tf.Variable(tf.zeros([3,1]), name=\"Weight_matrix\")\n",
    "bias = tf.Variable(tf.zeros([1]), name = \"bias\")\n",
    "Y = tf.placeholder(tf.float32,[None, 1], name = \"targets\")\n",
    "\n",
    "with tf.name_scope(\"Cubic_Model\") as scope:\n",
    "#     add the squares and the cubes of the variable\n",
    "    X2 = tf.square(X)\n",
    "    X3 = tf.multiply(X2,X)\n",
    "    bases = tf.concat([X3,X2,X], 1)\n",
    "    prediction = tf.matmul(bases,W) + bias\n",
    "#     define the cost function, SSE\n",
    "with tf.name_scope(\"cost\") as scope:\n",
    "    cost = tf.reduce_mean(tf.square(prediction - Y))\n",
    "# train using gradient descent\n",
    "with tf.name_scope(\"train\") as scope:\n",
    "    train_step = tf.train.AdamOptimizer(learn_rate).minimize(cost)\n",
    "\n",
    "# add summary operations\n",
    "# w_c = tf.summary.scalar(\"cost_function\", cost)\n",
    "# Merge all the summaries and write them out to log_dir\n",
    "# merged = tf.summary.merge_all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare fake data\n",
    "data = []\n",
    "targets = []\n",
    "for i in xrange(- num_datapoints, num_datapoints):\n",
    "    data.append([i])\n",
    "    targets.append([a*i*i*i + b*i*i + c*i + d])\n",
    "    \n",
    "\n",
    "def get_data(train = True, index = 0):\n",
    "    if train:\n",
    "        i = index\n",
    "        batch_start_idx = (i * batch_size) % (num_datapoints - batch_size)\n",
    "        batch_end_idx = batch_start_idx + batch_size\n",
    "        batch_xs = data[batch_start_idx:batch_end_idx]\n",
    "        batch_ys = targets[batch_start_idx:batch_end_idx]\n",
    "        xs = np.array(batch_xs)\n",
    "        ys = np.array(batch_ys)\n",
    "        return { X: xs, Y: ys }\n",
    "    else:\n",
    "        xs = np.array(data)\n",
    "        ys = np.array(targets)\n",
    "        return { X: xs, Y: ys }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Iteration: 0\n",
      " Iteration: 1000\n",
      " Iteration: 2000\n",
      " Iteration: 3000\n",
      " Iteration: 4000\n",
      " Iteration: 5000\n",
      " Iteration: 6000\n",
      " Iteration: 7000\n",
      " Iteration: 8000\n",
      " Iteration: 9000\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "verbose = False\n",
    "init = tf.global_variables_initializer()\n",
    "# training with mini batch gradient descent\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "    for i in range(steps):\n",
    "        sess.run(train_step, feed_dict=get_data(True, i))\n",
    "        if i%(steps/10) == 0 :\n",
    "            print(\" Iteration: %d\" % i)\n",
    "            if verbose == True:\n",
    "                print(\"W:\" ,  sess.run(W))\n",
    "                print(\"bias:\" , sess.run(bias))\n",
    "            \n",
    "    print(\"Training Complete\")\n",
    "    learned_weights =  sess.run(W)\n",
    "    learned_bias = sess.run(bias)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learned weights and bias should be: 1.000000 -4.000000 6.000000 -9.000000 \n",
      "learned weights and bias are\n",
      "1.00020027161\n",
      "-3.95895862579\n",
      "8.68481636047\n",
      "-16.8111\n"
     ]
    }
   ],
   "source": [
    "print(\"learned weights and bias should be: %f %f %f %f \"%(a,b,c,d))\n",
    "print(\"learned weights and bias are\")\n",
    "for w in learned_weights:\n",
    "    print np.asscalar(w)\n",
    "for b in learned_bias:\n",
    "    print b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
